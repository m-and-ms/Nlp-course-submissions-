# -*- coding: utf-8 -*-
"""assignment nlp1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NbYyMKGUnGrjHDXC4aq9kLKODTGLPoHx
"""

import re 
import nltk



nltk.download('punkt')
sentence="PresidentBarackObama met Microsoft founder [P]Bill Gates  yesterday." 
  
target="([^\s+^\W+][A-Z])"
p="[A-Z]" 

matches = re.findall(target, sentence)
print(matches)  
for case in matches:
   r=re.findall(p, case)[0]   
   sentence=(" "+r).join(sentence.split(r))
  
print(sentence)   
deci_pattern = "[\d+]+\.[0-9]{2}"

decimals = ["123.12", "2", "56754", "92929292929292.12", "0.21", "3.1"]
print("  ".join(decimals))

pattern = re.findall(deci_pattern, "  ".join(decimals))
checker =[("yes",i) for i,x in enumerate (decimals) if x in pattern ]
print (pattern)
print (checker)



paragraph="A girl with brown hair and brown eyes stood on the stage looking at the audience . This brief paragraph is used to give an example of an informal essay written about a person's dreams. :D  <3"

words=nltk.word_tokenize(paragraph)
print(words)

sent=nltk.sent_tokenize(paragraph)
print(sent)
from nltk.tokenize import TweetTokenizer

thing=nltk.TweetTokenizer().tokenize(paragraph)
print(thing)

regexer =nltk.regexp_tokenize(paragraph,p)
print(regexer)






import pickle



from google.colab import files
uploaded = files.upload()

from nltk.metrics import edit_distance
import pandas as pd
def spell_correction(document,vocab):######################with suggestions
  with open(vocab,'rb') as f:
    vocab = pickle.load(f)

  with open(document,'rb') as f:
    rawtext= pickle.load(f)


  tokens=nltk.WordPunctTokenizer().tokenize(rawtext)
  tokens=[x for x in tokens if x]  
  #print (tokens)
  #print(vocab)

  wrongwords=[word for word in tokens if word not in vocab]
  #print (wrongwords)
  error_location =[x for x ,_ in enumerate(tokens) if _ in wrongwords]
  #print(wrongwords)

  suggestions={}
  mindistances_sugg={}

  bestmatch={}
  for word in wrongwords:
    mindistances=[]
    mindistances_word=[]
    for v in vocab :
          
           if edit_distance(word, v) <= 4:
              mindistances.append(v)
              mindistances_word.append(edit_distance(word, v))

    
    suggestions[word]= mindistances 
    try:
     mindistances_sugg[word]= min(mindistances_word)
    except:
      pass
  #print(mindistances_word)
  
  for word in (wrongwords):
     dist=mindistances_sugg[word]
     key =[x for x in suggestions[word] if edit_distance(word, x)==dist ]
     bestmatch[word]=key
   
     #print(bestmatch) 

    
  
  big_table=list(zip(wrongwords,error_location,bestmatch.values()))
  
  df = pd.DataFrame(big_table)
  df.columns=["wrongspellings","location","correction"]
  #print (error_location)
  #print(big_table)
  
  for word in wrongwords :
     print (word +"....?" ,"did you mean .........",suggestions[word])
     print( "best match ......",bestmatch[word],"please do humanity a favor and go to school boy .....................")
    
    
  return df  




########################if we dont need suggestions:###############################yuhoooo
def find_correction(document ,vocab):
  with open(vocab,'rb') as f:
    vocab = pickle.load(f)
  
  with open(document,'rb') as f:
    rawtext= pickle.load(f)


  tokens=nltk.WordPunctTokenizer().tokenize(rawtext)
  tokens=[x for x in tokens if x]  
  #print (tokens)
  #print(vocab)

  wrongwords=[word for word in tokens if word not in vocab]
  #print (wrongwords)
  error_location =[x for x ,_ in enumerate(tokens) if _ in wrongwords]
  #print(wrongwords)
  bestmatch={}
  min_edit_d={}
  for word in wrongwords :
    min_edit_d[word]=len(word)
  for word in wrongwords:
  
    for v in vocab :
      
       if edit_distance(word, v) < min_edit_d[word]:
           min_edit_d[word]=edit_distance(word, v) 
           bestmatch[word]=v
       else :
        pass
      
      
  big_table=list(zip(wrongwords,error_location,bestmatch.values()))
  
  df = pd.DataFrame(big_table)
  df.columns=["wrongspellings","location","correction"]
  #print (error_location)
  #print(big_table)
  
  for word in wrongwords :
     print (word +"....?" )#,"did you mean .........",suggestions[word])
     print( "best match ......",bestmatch[word],"please do humanity a favor and go to school boy .....................")
  return df

print(find_correction("modemma.pkl","emmasvocab.pkl"))
print(spell_correction("modemma.pkl","emmasvocab.pkl"))
'''
#print(suggestions)


import nltk, re, string, collections
from nltk.util import ngrams

unigrams = ngrams(tokens, 1)
unigramFreq = collections.Counter(unigrams)
#unigramFreq.most_common()
'''

